{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MRI images: 1744\n",
      "Number of CT images: 1742\n",
      "Shape of an MRI image array: (280, 212, 3)\n",
      "Shape of a CT image array: (512, 512)\n",
      "Shape of a resized MRI image array: (256, 256)\n",
      "Shape of a resized CT image array: (256, 256)\n",
      "Minimum pixel value in a normalized MRI image: 0.0\n",
      "Maximum pixel value in a normalized MRI image: 0.8392156862745098\n",
      "Minimum pixel value in a normalized CT image: 0.0\n",
      "Maximum pixel value in a normalized CT image: 1.0\n",
      "Number of paired data samples: 1742\n",
      "Number of training samples: 1219\n",
      "Number of validation samples: 261\n",
      "Number of test samples: 262\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from preprocessing import train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture for the shared encoder\n",
    "class SharedEncoder(nn.Module):\n",
    "    def __init__(self, final_image_size, latent_dim):\n",
    "        super(SharedEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc = nn.Linear(256 * (final_image_size // 8) * (final_image_size // 8), latent_dim)  # Adjust the linear layer input size\n",
    "\n",
    "        self.final_image_size = final_image_size\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        encoded_features = self.fc(x)\n",
    "        return encoded_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedDecoder(nn.Module):\n",
    "    def __init__(self, final_image_size, latent_dim):\n",
    "        super(SharedDecoder, self).__init__()\n",
    "        self.final_image_size = final_image_size\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Calculate the number of output features from the Linear layer\n",
    "        num_features = 256 * (final_image_size // 8) * (final_image_size // 8)\n",
    "\n",
    "        # Adjust the Linear layer to match the encoder's output\n",
    "        self.fc = nn.Linear(latent_dim*2, num_features)\n",
    "\n",
    "        self.conv3 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), 256, self.final_image_size // 8, self.final_image_size // 8)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        decoded_image = torch.sigmoid(self.conv1(x))\n",
    "        return decoded_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cINN model\n",
    "class ConditionalINN(nn.Module):\n",
    "    def __init__(self, final_image_size, latent_dim):\n",
    "        super(ConditionalINN, self).__init__()\n",
    "        self.shared_encoder = SharedEncoder(final_image_size, latent_dim)\n",
    "        self.shared_decoder = SharedDecoder(final_image_size, latent_dim)\n",
    "        # Inside the forward method of ConditionalINN\n",
    "    def forward(self, x, direction):\n",
    "        if direction == 0:  # MRI to CT translation\n",
    "            encoded_features = self.shared_encoder(x)\n",
    "            noise = torch.randn(x.size(0), self.shared_encoder.latent_dim).to(x.device)\n",
    "\n",
    "            # Check the shapes of encoded_features and noise\n",
    "            combined_input = torch.cat((encoded_features, noise), dim=1)\n",
    "\n",
    "\n",
    "            # # Debug prints\n",
    "            # print(\"encoded_features shape:\", encoded_features.shape)\n",
    "            # print(\"noise shape:\", noise.shape)\n",
    "            # print(\"combined_input shape:\", combined_input.shape)\n",
    "\n",
    "            generated_image = self.shared_decoder(combined_input)\n",
    "\n",
    "\n",
    "\n",
    "            return generated_image\n",
    "        elif direction == 1:  # Add other translation directions if needed\n",
    "            # Add code for other translation directions here\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the cINN model\n",
    "final_image_size = 256 # ...  # Set the image size after convolutions\n",
    "latent_dim = 64 # ...  # Set the desired latent dimension\n",
    "cinn = ConditionalINN(final_image_size, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate PSNR\n",
    "def psnr(img1, img2):\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    max_pixel = 1.0  # Assuming pixel values are in [0, 1] range\n",
    "    psnr_value = 20 * torch.log10(max_pixel / torch.sqrt(mse))\n",
    "    return psnr_value.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def train_epoch(model, dataloader, optimizer, criterion):\\n    model.train()\\n    for batch_mri, batch_ct in dataloader:\\n        optimizer.zero_grad()\\n        # Perform MRI-to-CT translation\\n        generated_ct = model(batch_mri, direction=0)\\n        # Calculate the loss (e.g., reconstruction loss)\\n        loss = criterion(generated_ct, batch_ct)\\n\\n        # Backpropagation and optimization\\n        loss.backward()\\n        optimizer.step()'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training loop\n",
    "'''def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for batch_mri, batch_ct in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # Perform MRI-to-CT translation\n",
    "        generated_ct = model(batch_mri, direction=0)\n",
    "        # Calculate the loss (e.g., reconstruction loss)\n",
    "        loss = criterion(generated_ct, batch_ct)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(cinn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Training Loss: 0.0691, Training PSNR: 11.7736\n",
      "Epoch [1/20] - Validation Loss: 0.0624, Validation PSNR: 12.0673\n",
      "Epoch [2/20] - Training Loss: 0.0602, Training PSNR: 12.2129\n",
      "Epoch [2/20] - Validation Loss: 0.0621, Validation PSNR: 12.0888\n",
      "Epoch [3/20] - Training Loss: 0.0591, Training PSNR: 12.2912\n",
      "Epoch [3/20] - Validation Loss: 0.0597, Validation PSNR: 12.2481\n",
      "Epoch [4/20] - Training Loss: 0.0593, Training PSNR: 12.2773\n",
      "Epoch [4/20] - Validation Loss: 0.0603, Validation PSNR: 12.1974\n",
      "Epoch [5/20] - Training Loss: 0.0590, Training PSNR: 12.3026\n",
      "Epoch [5/20] - Validation Loss: 0.0598, Validation PSNR: 12.2416\n",
      "Epoch [6/20] - Training Loss: 0.0590, Training PSNR: 12.2982\n",
      "Epoch [6/20] - Validation Loss: 0.0604, Validation PSNR: 12.1908\n",
      "Epoch [7/20] - Training Loss: 0.0590, Training PSNR: 12.2973\n",
      "Epoch [7/20] - Validation Loss: 0.0597, Validation PSNR: 12.2508\n",
      "Epoch [8/20] - Training Loss: 0.0592, Training PSNR: 12.2833\n",
      "Epoch [8/20] - Validation Loss: 0.0594, Validation PSNR: 12.2701\n",
      "Epoch [9/20] - Training Loss: 0.0588, Training PSNR: 12.3067\n",
      "Epoch [9/20] - Validation Loss: 0.0595, Validation PSNR: 12.2613\n",
      "Epoch [10/20] - Training Loss: 0.0591, Training PSNR: 12.2938\n",
      "Epoch [10/20] - Validation Loss: 0.0597, Validation PSNR: 12.2458\n",
      "Epoch [11/20] - Training Loss: 0.0583, Training PSNR: 12.3548\n",
      "Epoch [11/20] - Validation Loss: 0.0617, Validation PSNR: 12.1077\n",
      "Epoch [12/20] - Training Loss: 0.0582, Training PSNR: 12.3564\n",
      "Epoch [12/20] - Validation Loss: 0.0610, Validation PSNR: 12.1541\n",
      "Epoch [13/20] - Training Loss: 0.0586, Training PSNR: 12.3263\n",
      "Epoch [13/20] - Validation Loss: 0.0616, Validation PSNR: 12.1122\n",
      "Epoch [14/20] - Training Loss: 0.0578, Training PSNR: 12.3878\n",
      "Epoch [14/20] - Validation Loss: 0.0613, Validation PSNR: 12.1274\n",
      "Epoch [15/20] - Training Loss: 0.0571, Training PSNR: 12.4402\n",
      "Epoch [15/20] - Validation Loss: 0.0613, Validation PSNR: 12.1259\n",
      "Epoch [16/20] - Training Loss: 0.0562, Training PSNR: 12.5119\n",
      "Epoch [16/20] - Validation Loss: 0.0621, Validation PSNR: 12.0806\n",
      "Epoch [17/20] - Training Loss: 0.0552, Training PSNR: 12.5882\n",
      "Epoch [17/20] - Validation Loss: 0.0624, Validation PSNR: 12.0546\n",
      "Epoch [18/20] - Training Loss: 0.0538, Training PSNR: 12.6975\n",
      "Epoch [18/20] - Validation Loss: 0.0622, Validation PSNR: 12.0947\n",
      "Epoch [19/20] - Training Loss: 0.0531, Training PSNR: 12.7591\n",
      "Epoch [19/20] - Validation Loss: 0.0648, Validation PSNR: 11.8923\n",
      "Epoch [20/20] - Training Loss: 0.0511, Training PSNR: 12.9240\n",
      "Epoch [20/20] - Validation Loss: 0.0652, Validation PSNR: 11.8642\n"
     ]
    }
   ],
   "source": [
    "model=cinn\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_psnr = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for batch_mri, batch_ct in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Perform MRI-to-CT translation\n",
    "        generated_ct = model(batch_mri, direction=0)\n",
    "        # Calculate the loss (e.g., reconstruction loss)\n",
    "        loss = criterion(generated_ct, batch_ct)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate and accumulate PSNR\n",
    "        psnr_value = psnr(generated_ct, batch_ct)\n",
    "        total_psnr += psnr_value\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate average loss and PSNR for the training dataset\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_psnr = total_psnr / num_batches\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Training Loss: {avg_loss:.4f}, Training PSNR: {avg_psnr:.4f}\")\n",
    "\n",
    "    # Validation loop (similar to training loop)\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_psnr = 0.0\n",
    "    num_val_batches = len(val_loader)\n",
    "\n",
    "    for val_batch_mri, val_batch_ct in val_loader:\n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            # Perform MRI-to-CT translation\n",
    "            val_generated_ct = model(val_batch_mri, direction=0)\n",
    "            # Calculate the validation loss\n",
    "            val_loss = criterion(val_generated_ct, val_batch_ct)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            # Calculate and accumulate PSNR for validation\n",
    "            val_psnr_value = psnr(val_generated_ct, val_batch_ct)\n",
    "            total_val_psnr += val_psnr_value\n",
    "\n",
    "    # Calculate average validation loss and PSNR\n",
    "    avg_val_loss = total_val_loss / num_val_batches\n",
    "    avg_val_psnr = total_val_psnr / num_val_batches\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Validation Loss: {avg_val_loss:.4f}, Validation PSNR: {avg_val_psnr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_mri = Image.open(\"dataset/images/trainB/mri12.jpg\")\n",
    "target_size = (256, 256)\n",
    "input_mri=input_mri.resize(target_size).convert('L')\n",
    "\n",
    "# Convert the input image to a PyTorch tensor\n",
    "input_mri = transforms.ToTensor()(input_mri)\n",
    "input_mri = input_mri.unsqueeze(0)\n",
    "\n",
    "# Perform MRI-to-CT translation\n",
    "generated_ct = cinn(input_mri, direction=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use or save the 'generated_ct' image as needed\n",
    "# save the generated image in the new output folder\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output folder\n",
    "output_folder = \"output\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# save the generated image\n",
    "save_image(generated_ct, os.path.join(output_folder, \"generated_ct.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BTP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
